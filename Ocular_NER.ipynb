{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ocular_NER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1tlMZsq4G30BREHKw5L28b4Z7xumEh80L",
      "authorship_tag": "ABX9TyNnC19hKuB0bSu025TYISdV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aakashagarwal6898/Emotion-Detection-using-Keras/blob/master/Ocular_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XltqlV6H-FR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install spacy==2.1.4 #TO AVOID CASCADED ENTITY  ERROR\n",
        "#!pip install -U spacy[cuda100]==2.1.1 #GPU enabled spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxvpIjArITz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import random\n",
        "import logging\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from spacy.gold import GoldParse\n",
        "from spacy.scorer import Scorer\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re\n",
        "import spacy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2evxvafEJpZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_dataturks_to_spacyy(dataturks_JSON_FilePath):\n",
        "\n",
        "    try:\n",
        "        training_data = []\n",
        "        lines=[]\n",
        "        with open(dataturks_JSON_FilePath, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            data = json.loads(line)\n",
        "            text = data['content']\n",
        "            entities = []\n",
        "            data_annotations = data['annotation']\n",
        "            if data_annotations is not None:\n",
        "                for annotation in data_annotations:\n",
        "                    #only a single point in text annotation.\n",
        "                    point = annotation['points'][0]\n",
        "                    labels = annotation['label']\n",
        "                    # handle both list of labels or a single label.\n",
        "                    if not isinstance(labels, list):\n",
        "                        labels = [labels]\n",
        "\n",
        "                    for label in labels:\n",
        "                        point_start = point['start']\n",
        "                        point_end = point['end']\n",
        "                        point_text = point['text']\n",
        "                        \n",
        "                        lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
        "                        rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
        "                        if lstrip_diff != 0:\n",
        "                            point_start = point_start + lstrip_diff\n",
        "                        if rstrip_diff != 0:\n",
        "                            point_end = point_end - rstrip_diff\n",
        "                        entities.append((point_start, point_end + 1 , label))\n",
        "            training_data.append((text, {\"entities\" : entities}))\n",
        "        return training_data\n",
        "    except Exception as e:\n",
        "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
        "        return None\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRwrwPnqO9z9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def trim_entity_spans(data: list) -> list:\n",
        "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
        "\n",
        "    Args:\n",
        "        data (list): The data to be cleaned in spaCy JSON format.\n",
        "\n",
        "    Returns:\n",
        "        list: The cleaned data.\n",
        "    \"\"\"\n",
        "    invalid_span_tokens = re.compile(r'\\s')\n",
        "\n",
        "    cleaned_data = []\n",
        "    for text, annotations in data:\n",
        "        entities = annotations['entities']\n",
        "        valid_entities = []\n",
        "        for start, end, label in entities:\n",
        "            valid_start = start\n",
        "            valid_end = end\n",
        "            while valid_start < len(text) and invalid_span_tokens.match(\n",
        "                    text[valid_start]):\n",
        "                valid_start += 1\n",
        "            while valid_end > 1 and invalid_span_tokens.match(\n",
        "                    text[valid_end - 1]):\n",
        "                valid_end -= 1\n",
        "            valid_entities.append([valid_start, valid_end, label])\n",
        "        cleaned_data.append([text, {'entities': valid_entities}])\n",
        "\n",
        "    return cleaned_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9qCIDCDNfkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "import random\n",
        "\n",
        "def train_spacy():\n",
        "\n",
        "    TRAIN_DATA = trim_entity_spans(convert_dataturks_to_spacyy(\"/content/ADIL_dataset_800.json\"))\n",
        "    nlp = spacy.blank('en')  # create blank Language class\n",
        "    # create the built-in pipeline components and add them to the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if 'ner' not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe('ner')\n",
        "        nlp.add_pipe(ner, last=True)\n",
        "       \n",
        "\n",
        "    # add labels\n",
        "    for _, annotations in TRAIN_DATA:\n",
        "         for ent in annotations.get('entities'):\n",
        "            ner.add_label(ent[2])\n",
        "\n",
        "    # get names of other pipes to disable them during training\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "        optimizer = nlp.begin_training()\n",
        "        for itn in range(20):\n",
        "            print(\"Starting iteration \" + str(itn))\n",
        "            random.shuffle(TRAIN_DATA)\n",
        "            losses = {}\n",
        "            for text, annotations in TRAIN_DATA:\n",
        "                nlp.update(\n",
        "                    [text],  # batch of texts\n",
        "                    [annotations],  # batch of annotations\n",
        "                    drop=0.3,  # dropout - make it harder to memorise data\n",
        "                    sgd=optimizer,  # callable to update weights\n",
        "                    losses=losses)\n",
        "            print(losses)\n",
        "            nlp.to_disk(\"/content/drive/My Drive/ocular_final_training/ner_100\")\n",
        "    return nlp\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLIYDBNWW_jk",
        "colab_type": "code",
        "outputId": "e2ff9289-dddc-46ae-985a-16fe6d62d2ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "nlpModel = train_spacy()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting iteration 0\n",
            "{'ner': 30550.171418521782}\n",
            "Starting iteration 1\n",
            "{'ner': 20927.656434691595}\n",
            "Starting iteration 2\n",
            "{'ner': 17521.76404734125}\n",
            "Starting iteration 3\n",
            "{'ner': 14847.324780167994}\n",
            "Starting iteration 4\n",
            "{'ner': 12927.412013169069}\n",
            "Starting iteration 5\n",
            "{'ner': 11870.408442600332}\n",
            "Starting iteration 6\n",
            "{'ner': 11260.5792119189}\n",
            "Starting iteration 7\n",
            "{'ner': 11004.502749313011}\n",
            "Starting iteration 8\n",
            "{'ner': 9732.409349565587}\n",
            "Starting iteration 9\n",
            "{'ner': 9114.690577490934}\n",
            "Starting iteration 10\n",
            "{'ner': 8427.844530666303}\n",
            "Starting iteration 11\n",
            "{'ner': 8907.852352691636}\n",
            "Starting iteration 12\n",
            "{'ner': 8593.68406971053}\n",
            "Starting iteration 13\n",
            "{'ner': 7857.249988236488}\n",
            "Starting iteration 14\n",
            "{'ner': 7998.509769759586}\n",
            "Starting iteration 15\n",
            "{'ner': 7160.963743739495}\n",
            "Starting iteration 16\n",
            "{'ner': 7469.112571175714}\n",
            "Starting iteration 17\n",
            "{'ner': 6696.6775124185615}\n",
            "Starting iteration 18\n",
            "{'ner': 6325.37412433198}\n",
            "Starting iteration 19\n",
            "{'ner': 6526.7565269039}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJVcqqJzmiKp",
        "colab_type": "code",
        "outputId": "1fa98667-b0ab-43af-a4fb-60b6d9bfe85d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# Opening txt file\n",
        "data = \"/content/apred-test.txt\"\n",
        "f = open(data, \"r\")\n",
        "# Storing data in variable\n",
        "textToPredict = f.read()\n",
        "# Sending textual data to Spacy model for NER\n",
        "doc = nlpModel(textToPredict)\n",
        "max_amt = 0\n",
        "i = 1\n",
        "data = {}\n",
        "items_list = []\n",
        "# Iterating over every entitiy to create a dictionary\n",
        "for ent in doc.ents:\n",
        "  # Saving only one instance of Total Bill Amount\n",
        "  if (ent.label_ == \"Total bill amount\"):\n",
        "    try:\n",
        "      amt = float(ent.text)\n",
        "      if amt > max_amt:\n",
        "        data[\"Total bill amount\"] = amt\n",
        "    except Exception as e:\n",
        "      pass\n",
        "  # Creating a list of Items\n",
        "  elif (ent.label_ == \"Items\"):\n",
        "    try:\n",
        "      items_list.append(ent.text)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "  # Checking if the detected key is already present in the key,\n",
        "  # If yes then we create a new key to store that value instead of overwriting the previous one\n",
        "  else:\n",
        "    if ent.label_ in data.keys():\n",
        "      data[ent.label_+\"-\"+str(i)] = ent.text\n",
        "      i +=1\n",
        "    else:\n",
        "      data[ent.label_] = ent.text\n",
        "# Staring the list of items using the Items key in the dictionary\n",
        "data[\"Items\"]=items_list\n",
        "# Sorting all the elements of the dictionary\n",
        "data = dict(sorted(data.items()))\n",
        "# Printing final result\n",
        "print(json.dumps(data, indent=2))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"Date\": \"04/13/18\",\n",
            "  \"Invoice number\": \"415\",\n",
            "  \"Items\": [\n",
            "    \"DINNER RODIZIO\",\n",
            "    \"ROTI\",\n",
            "    \"Paneer\",\n",
            "    \"Kheer\",\n",
            "    \"coca cola\"\n",
            "  ],\n",
            "  \"Store address\": \"2023 South Pine Avenue\\nOcala, Florida 34471\",\n",
            "  \"Store name\": \"Brazilian Steak House\",\n",
            "  \"Time\": \"09:16 pm\"\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}